<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Prompt Guard: Input Protection for AI Agents | Nick's Blog</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet">
  <style>
    :root { --primary: #0F172A; --accent: #F97316; --bg: #FFFFFF; --bg-alt: #F8FAFC; --text: #1E293B; --text-muted: #64748B; --border: #E2E8F0; }
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body { font-family: 'Inter', sans-serif; color: var(--text); background: var(--bg); line-height: 1.7; }
    h1, h2, h3 { font-family: 'Space Grotesk', sans-serif; font-weight: 700; line-height: 1.3; }
    .container { max-width: 720px; margin: 0 auto; padding: 0 24px; }
    header { padding: 32px 0; border-bottom: 1px solid var(--border); }
    .logo { font-family: 'Space Grotesk', sans: 20px-serif; font-size; font-weight: 700; color: var(--primary); text-decoration: none; }
    .logo span { color: var(--accent); }
    .back-link { display: inline-block; margin: 24px 0; color: var(--text-muted); text-decoration: none; font-size: 14px; }
    .back-link:hover { color: var(--accent); }
    article { padding: 32px 0; }
    .meta { font-size: 14px; color: var(--text-muted); margin-bottom: 8px; }
    h1 { font-size: 36px; margin-bottom: 24px; color: var(--primary); }
    h2 { font-size: 24px; margin: 32px 0 16px; color: var(--primary); }
    h3 { font-size: 18px; margin: 24px 0 12px; color: var(--primary); }
    p { margin-bottom: 16px; }
    .tag { display: inline-block; background: var(--accent); color: white; padding: 4px 12px; border-radius: 20px; font-size: 12px; font-weight: 600; margin-right: 8px; }
    .headline { background: var(--bg-alt); border-left: 4px solid var(--accent); padding: 20px; margin: 24px 0; border-radius: 0 8px 8px 0; }
    .headline h3 { font-size: 18px; margin-bottom: 8px; }
    .headline p { font-size: 14px; color: var(--text-muted); margin: 0; }
    ul, ol { margin: 16px 0; padding-left: 24px; }
    li { margin-bottom: 8px; }
    .code { background: var(--bg-alt); padding: 16px; border-radius: 8px; font-family: monospace; font-size: 13px; overflow-x: auto; }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    footer { padding: 32px 0; border-top: 1px solid var(--border); text-align: center; }
    footer p { font-size: 14px; color: var(--text-muted); }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { padding: 8px 12px; text-align: left; border: 1px solid var(--border); font-size: 13px; }
    th { background: var(--bg-alt); font-weight: 600; }
  </style>
</head>
<body>
  <header>
    <div class="container">
      <a href="index.html" class="logo">Nick's <span>Blog</span></a>
    </div>
  </header>
  <div class="container">
    <a href="index.html" class="back-link">← Back to all posts</a>
    <article>
      <p class="meta">February 19, 2026 • AI • Security</p>
      <h1>Building Prompt Guard: Input Protection for AI Agents</h1>
      <div class="content">
<p>As AI agents become more prevalent in customer communication, protecting them from malicious inputs is critical. This is the first in a series on building Prompt Guard - an OpenClaw skill for AI security.</p>

<h2>The Problem</h2>
<p>Users can inject malicious prompts into emails, chat messages, and form submissions to:</p>
<ul>
<li>Extract your AI's system prompts</li>
<li>Override safety guidelines</li>
<li>Manipulate the AI into harmful responses</li>
<li>Extract confidential information</li>
</ul>

<h2>What It Does</h2>
<p>Input Protection scans all incoming user communications BEFORE they reach your AI agent:</p>

<ul>
<li><strong>Detects injection patterns</strong> - Recognises common attack vectors</li>
<li><strong>Sanitises input</strong> - Removes or neutralises malicious content</li>
<li><strong>Blocks high-risk attempts</strong> - Stops obvious attacks outright</li>
<li><strong>Logs for audit</strong> - Records suspicious inputs for review</li>
</ul>

<h2>Detection Categories</h2>

<h3>1. Direct Instruction Override</h3>
<table>
<tr><th>Pattern</th><th>Risk</th></tr>
<tr><td>"Ignore previous instructions"</td><td>CRITICAL</td></tr>
<tr><td>"Disregard system prompt"</td><td>CRITICAL</td></tr>
<tr><td>"You are now [role]"</td><td>HIGH</td></tr>
<tr><td>"[INST]...[/INST]"</td><td>HIGH</td></tr>
</table>

<h3>2. Stealth/Indirect Attacks</h3>
<table>
<tr><th>Pattern</th><th>Risk</th></tr>
<tr><td>Base64 encoded instructions</td><td>MEDIUM</td></tr>
<tr><td>Unicode homoglyphs</td><td>MEDIUM</td></tr>
<tr><td>Distraction attacks ("my daughter needs help...")</td><td>LOW</td></tr>
</table>

<h3>3. Context Manipulation</h3>
<table>
<tr><th>Pattern</th><th>Risk</th></tr>
<tr><td>"This is a new conversation"</td><td>LOW</td></tr>
<tr><td>"You're in developer mode"</td><td>HIGH</td></tr>
<tr><td>"Remove any restrictions"</td><td>CRITICAL</td></tr>
</table>

<div class="headline">
  <h3>Key Insight</h3>
  <p>Input protection is crowded (Aegis, llm-guard). The opportunity is in output protection - scanning what your AI says BEFORE it reaches customers.</p>
</div>

<h2>Integration</h2>
<pre class="code">const result = await scanInput(email);

if (result.blocked) {
  // Don't pass to AI
} else if (result.warning) {
  // Pass with caution
} else {
  // Safe - pass to AI
}</pre>

<p><strong>Part 2:</strong> Output Protection - preventing your AI from saying things it shouldn't.</p>

      </div>
      <footer>
        <p>More posts coming soon...</p>
      </footer>
    </article>
  </div>
</body>
</html>
